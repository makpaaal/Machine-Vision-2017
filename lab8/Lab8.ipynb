{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pattern recognition: Lab 8\n",
    "### Tasks:\n",
    "* Plot the error\n",
    "* Model XOR with the help of sigmoid\n",
    "* Add Nesterov rule to learning equation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "k = 1\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-k*x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - x**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n",
      "[0 0] [  4.99812582e-05]\n",
      "[0 1] [ 0.99735084]\n",
      "[1 0] [ 0.99581505]\n",
      "[1 1] [-0.00154635]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = tanh\n",
    "        self.activation_prime = tanh_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        \n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "        # output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "         \n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "\n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "\n",
    "            if k % 10000 == 0: \n",
    "                print('epochs:', k)\n",
    "\n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "#     X = np.array([[-1, -1],\n",
    "#                   [-1, 1],\n",
    "#                   [1, -1],\n",
    "#                   [1, 1]])\n",
    "#     y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epochs: 0\n",
      "epochs: 10000\n",
      "epochs: 20000\n",
      "epochs: 30000\n",
      "epochs: 40000\n",
      "epochs: 50000\n",
      "epochs: 60000\n",
      "epochs: 70000\n",
      "epochs: 80000\n",
      "epochs: 90000\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYYAAAEKCAYAAAAW8vJGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x85644670f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.0068887   0.00414707  0.00413085]\n",
      "[0 1] [ 0.98104681  0.98656568  0.98598949]\n",
      "[1 0] [ 0.98641638  0.98918333  0.99218288]\n",
      "[1 1] [ 0.02054557  0.01372552  0.01291365]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as pltimg\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "# Set weights\n",
    "        self.weights = []\n",
    "# layers = [2,2,1]\n",
    "# range of weight values (-1,1)\n",
    "# input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "# output layer - random((2+1, 1)) : 3 x 1\n",
    "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
    "        self.weights.append(r)\n",
    "\n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000):\n",
    "# Add column of ones to X\n",
    "# This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "        myList=[]\n",
    "        avList=[]\n",
    "\n",
    "\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            a = [X[i]]\n",
    "    \n",
    "            for l in range(len(self.weights)):\n",
    "                dot_value = np.dot(a[l], self.weights[l])\n",
    "                activation = self.activation(dot_value)\n",
    "                a.append(activation)\n",
    "# output layer\n",
    "\n",
    "\n",
    "            error = y[i] - a[-1]\n",
    "            myList.append(np.sum(error**2))# mean squard error MSE\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# we need to begin at the second to last layer\n",
    "# (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1):\n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "# reverse\n",
    "# [level3(output)->level2(hidden)] => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "# backpropagation\n",
    "# 1. Multiply its output delta and input activation\n",
    "# to get the gradient of the weight.\n",
    "# 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "\n",
    "            cnt = 0\n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                cnt =cnt + layer.T.dot(delta)\n",
    "                self.weights[i] =self.weights[i]+ learning_rate * cnt#  momentum\n",
    "            t = np.average(myList)\n",
    "            avList.append(t)\n",
    "            if k % 10000 == 0:\n",
    "                print('epochs:', k)\n",
    "                t = np.average(myList)\n",
    "                avList.append(t)\n",
    "\n",
    "# plt.show()\n",
    "#print(myList)\n",
    "\n",
    "#plt.plot(myList[1])\n",
    "        plt.plot(avList)\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('error')\n",
    "        plt.show()\n",
    "\n",
    "    def predict(self, x):\n",
    "\n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))\n",
    "\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1,0])\n",
    "# X = np.array([[-1, -1],\n",
    "# [-1, 1],\n",
    "# [1, -1],\n",
    "# [1, 1]])\n",
    "# y = np.array([0, 1, 1, 0])\n",
    "\n",
    "    nn.fit(X, y)\n",
    "    for e in X:\n",
    "        print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0] [ 0.01181863]\n",
      "[0 1] [ 0.9846658]\n",
      "[1 0] [ 0.98127838]\n",
      "[1 1] [ 0.01855562]\n",
      "[0 0] [ 0.00766718]\n",
      "[0 1] [ 0.98957725]\n",
      "[1 0] [ 0.9871989]\n",
      "[1 1] [ 0.01285688]\n",
      "[0 0] [ 0.00597838]\n",
      "[0 1] [ 0.99156175]\n",
      "[1 0] [ 0.989689]\n",
      "[1 1] [ 0.010409]\n",
      "[0 0] [ 0.00503782]\n",
      "[0 1] [ 0.99275726]\n",
      "[1 0] [ 0.99117968]\n",
      "[1 1] [ 0.00904539]\n",
      "[0 0] [ 0.00441076]\n",
      "[0 1] [ 0.99356995]\n",
      "[1 0] [ 0.99214994]\n",
      "[1 1] [ 0.00810175]\n",
      "[0 0] [ 0.00394805]\n",
      "[0 1] [ 0.99413537]\n",
      "[1 0] [ 0.99285495]\n",
      "[1 1] [ 0.00738584]\n",
      "[0 0] [ 0.00360109]\n",
      "[0 1] [ 0.99458599]\n",
      "[1 0] [ 0.99340535]\n",
      "[1 1] [ 0.00684701]\n",
      "[0 0] [ 0.00332704]\n",
      "[0 1] [ 0.99495408]\n",
      "[1 0] [ 0.99383891]\n",
      "[1 1] [ 0.00641124]\n",
      "[0 0] [ 0.00309519]\n",
      "[0 1] [ 0.99523276]\n",
      "[1 0] [ 0.99419698]\n",
      "[1 1] [ 0.00603366]\n",
      "[0 0] [ 0.00291403]\n",
      "[0 1] [ 0.99549449]\n",
      "[1 0] [ 0.99450943]\n",
      "[1 1] [ 0.00573975]\n",
      "[0 0] [ 0.00275372]\n",
      "[0 1] [ 0.99570751]\n",
      "[1 0] [ 0.99477279]\n",
      "[1 1] [ 0.00547697]\n",
      "[0 0] [ 0.00261619]\n",
      "[0 1] [ 0.99589348]\n",
      "[1 0] [ 0.9950031]\n",
      "[1 1] [ 0.0052484]\n",
      "[0 0] [ 0.00249886]\n",
      "[0 1] [ 0.99606462]\n",
      "[1 0] [ 0.99521091]\n",
      "[1 1] [ 0.00505699]\n",
      "[0 0] [ 0.0023891]\n",
      "[0 1] [ 0.99620452]\n",
      "[1 0] [ 0.99538536]\n",
      "[1 1] [ 0.00486857]\n",
      "[0 0] [ 0.00229601]\n",
      "[0 1] [ 0.99634111]\n",
      "[1 0] [ 0.99554708]\n",
      "[1 1] [ 0.00471185]\n",
      "[0 0] [ 0.00220523]\n",
      "[0 1] [ 0.99644897]\n",
      "[1 0] [ 0.99568573]\n",
      "[1 1] [ 0.00455356]\n",
      "[0 0] [ 0.0021239]\n",
      "[0 1] [ 0.99655177]\n",
      "[1 0] [ 0.99580824]\n",
      "[1 1] [ 0.00440986]\n",
      "[0 0] [ 0.00205731]\n",
      "[0 1] [ 0.99665597]\n",
      "[1 0] [ 0.99593387]\n",
      "[1 1] [ 0.00429514]\n",
      "[0 0] [ 0.00199447]\n",
      "[0 1] [ 0.99675117]\n",
      "[1 0] [ 0.99604543]\n",
      "[1 1] [ 0.00418731]\n",
      "[0 0] [ 0.00193371]\n",
      "[0 1] [ 0.99683151]\n",
      "[1 0] [ 0.99614589]\n",
      "[1 1] [ 0.00408025]\n",
      "[0 0] [ 0.00187944]\n",
      "[0 1] [ 0.99690822]\n",
      "[1 0] [ 0.99624139]\n",
      "[1 1] [ 0.00398353]\n",
      "[0 0] [ 0.00182983]\n",
      "[0 1] [ 0.9969832]\n",
      "[1 0] [ 0.99633054]\n",
      "[1 1] [ 0.00389718]\n",
      "[0 0] [ 0.00178274]\n",
      "[0 1] [ 0.99705173]\n",
      "[1 0] [ 0.9964103]\n",
      "[1 1] [ 0.00381208]\n",
      "[0 0] [ 0.00173912]\n",
      "[0 1] [ 0.99711672]\n",
      "[1 0] [ 0.99648608]\n",
      "[1 1] [ 0.00373426]\n",
      "[0 0] [ 0.00170091]\n",
      "[0 1] [ 0.99718073]\n",
      "[1 0] [ 0.99656296]\n",
      "[1 1] [ 0.00366743]\n",
      "[0 0] [ 0.00166413]\n",
      "[0 1] [ 0.99723992]\n",
      "[1 0] [ 0.99663385]\n",
      "[1 1] [ 0.00360293]\n",
      "[0 0] [ 0.00162616]\n",
      "[0 1] [ 0.99728882]\n",
      "[1 0] [ 0.99669627]\n",
      "[1 1] [ 0.00353367]\n",
      "[0 0] [ 0.00159032]\n",
      "[0 1] [ 0.99733539]\n",
      "[1 0] [ 0.9967539]\n",
      "[1 1] [ 0.00346698]\n",
      "[0 0] [ 0.00155603]\n",
      "[0 1] [ 0.99737822]\n",
      "[1 0] [ 0.99680947]\n",
      "[1 1] [ 0.00340415]\n",
      "[0 0] [ 0.00152644]\n",
      "[0 1] [ 0.99742458]\n",
      "[1 0] [ 0.99686502]\n",
      "[1 1] [ 0.00335014]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1.0 + np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return x*(1.0-x)\n",
    "\n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "\n",
    "def tanh_prime(x):\n",
    "    return 1.0 - (x)**2\n",
    "\n",
    "\n",
    "class NeuralNetwork:\n",
    "\n",
    "    def __init__(self, layers):\n",
    "        self.errors = []\n",
    "        self.activation = sigmoid\n",
    "        self.activation_prime = sigmoid_prime\n",
    "\n",
    "        # Set weights\n",
    "        self.weights = []\n",
    "        # layers = [2,2,1]\n",
    "        # range of weight values (-1,1)\n",
    "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
    "        for i in range(1, len(layers) - 1):\n",
    "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
    "            self.weights.append(r)\n",
    "           \n",
    "            # output layer - random((2+1, 1)) : 3 x 1\n",
    "            r = 2*np.random.random( (layers[i]+1 , layers[i+1])) -1\n",
    "            self.weights.append(r)\n",
    "#         print(self.weights) \n",
    "    def fit(self, X, y, learning_rate=0.2, epochs=100000 ,momentum = 0.4):\n",
    "        # Add column of ones to X\n",
    "        # This is to add the bias unit to the input layer\n",
    "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
    "        \n",
    "        X = np.concatenate((ones.T, X), axis=1)\n",
    "#         i = np.random.randint(X.shape[0])\n",
    "            \n",
    "#         b = [X[i]]\n",
    "#         print(\"Printing B:\")\n",
    "#         print(b)\n",
    "#         print(\"self weights\")\n",
    "#         print(self.weights)\n",
    "#         for l in range(len(self.weights)):\n",
    "#                     dot_value = np.dot(b[l], self.weights[l])\n",
    "                    \n",
    "#                     activation = self.activation(dot_value)\n",
    "#                     print(\"activation:\")\n",
    "#                     print(activation)\n",
    "#                     b.append(activation)\n",
    "#         print(\"last:\")\n",
    "#         print(b)\n",
    "#         print(len(b))\n",
    "#         error1 = y[i] - b[-1]\n",
    "#         deltas1 = [error1 * self.activation_prime(b[-1])]\n",
    "#         print(deltas1)\n",
    "#         deltas1.append(deltas1[-1].dot(self.weights[1].T))\n",
    "#         deltas1.reverse()\n",
    "#         print(deltas1)\n",
    "#         print(\"Updated weights:\")\n",
    "#         for i in range(len(self.weights)):\n",
    "#                 layer1 = np.atleast_2d(b[i])\n",
    "#                 delta1 = np.atleast_2d(deltas1[i])\n",
    "#                 self.weights[i] += learning_rate * layer1.T.dot(delta1)\n",
    "#                 print(\"layer\")\n",
    "#                 print(layer1.T)\n",
    "#         print(self.weights)   \n",
    "        #this is end\n",
    "        for k in range(epochs):\n",
    "            i = np.random.randint(X.shape[0])\n",
    "            \n",
    "            a = [X[i]]\n",
    "            \n",
    "            for l in range(len(self.weights)):\n",
    "                    dot_value = np.dot(a[l], self.weights[l])\n",
    "                    \n",
    "                    activation = self.activation(dot_value)\n",
    "                    a.append(activation)\n",
    "            # output layer\n",
    "            error = y[i] - a[-1]\n",
    "#             self.errors.append(error**2)/(k)\n",
    "            deltas = [error * self.activation_prime(a[-1])]\n",
    "#             plt.plot(deltas)\n",
    "#             plt.axis()\n",
    "            # we need to begin at the second to last layer \n",
    "            # (a layer before the output layer)\n",
    "            for l in range(len(a) - 2, 0, -1): \n",
    "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
    "\n",
    "            # reverse\n",
    "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
    "            deltas.reverse()\n",
    "\n",
    "            # backpropagation\n",
    "            # 1. Multiply its output delta and input activation \n",
    "            #    to get the gradient of the weight.\n",
    "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
    "            prev_weights_delta = []\n",
    "            prev_3_1 = np.zeros((3,1))\n",
    "            prev_3_3 = np.zeros((3,3))\n",
    "            prev_weights_delta.append(prev_3_3)\n",
    "            prev_weights_delta.append(prev_3_1)\n",
    "            \n",
    "            for i in range(len(self.weights)):\n",
    "                layer = np.atleast_2d(a[i])\n",
    "                delta = np.atleast_2d(deltas[i])\n",
    "                #self.weights[i] += learning_rate * layer.T.dot(delta)\n",
    "                \n",
    "                delta = learning_rate * (layer.T.dot(delta)-(momentum * prev_weights_delta[i]))\n",
    "                self.weights[i] += delta\n",
    "                self.weights[i] += momentum * prev_weights_delta[i]\n",
    "                prev_weights_delta[i] = delta\n",
    "#             if k % 10000 == 0: \n",
    "#                 print('epochs:', k)\n",
    "            \n",
    "    def predict(self, x): \n",
    "    \n",
    "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
    "#          print(a)\n",
    "        for l in range(0, len(self.weights)):\n",
    "            a = self.activation(np.dot(a, self.weights[l]))\n",
    "        return a\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    nn = NeuralNetwork([2,2,1])\n",
    "    X = np.array([[0, 0],\n",
    "                  [0, 1],\n",
    "                  [1, 0],\n",
    "                  [1, 1]])\n",
    "    y = np.array([0, 1, 1, 0])\n",
    "    for it in range(30):\n",
    "        nn.fit(X, y)\n",
    "        for e in X:\n",
    "            print(e,nn.predict(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
